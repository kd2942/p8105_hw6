---
title: "p8105 Hw#6"
author: "Kaylin De Silva"
date: 12-02-2024
output: github_document
---
```{r load libraries}
#loading libraries and setting seed for reproducibility  
library(tidyverse)
library(dplyr)
library(rvest)
library(modelr)
library(mgcv)
library("SemiPar")
set.seed(1)
```
This chunk loads the tidyverse, rvest, and dplyr libraries and fixes the output. 

**Problem 1**
```{r loading datset problem 1}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r bootstrapping}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_sample(weather_df) |> 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm")

boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

boot_straps
```

```{r}
boot_straps |> 
  slice(1:5) |> 
  unnest(strap_sample) |> 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm", se = FALSE) +
  facet_grid(~strap_number) 
```

```{r summarizing bootstrapping results}
bootstrap_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin, data = df) ),
    results = map(models, broom::tidy)) |> 
  select(-strap_sample, -models) |> 
  unnest(results) 

bootstrap_results |> 
  group_by(term) |> 
  summarize(
    boot_se = sd(estimate),
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975))|> 
  knitr::kable(digits = 3)
```
__Unfortutantely did not have time to complete problem #1__

**Problem 2**
```{r loading dataset problem 2}
#loading csv
raw_washington_df = read.csv(file = "./homicide-data.csv")

#viewing variables
head(raw_washington_df)
```
The data set has 52,179 observations and 12 columns. 

```{r manipulating data by creating variables and filtering observations as needed}
#creating a city_state variable and removing cities and races that are not needed for analysis
washington_df = raw_washington_df |>
  mutate(
    city_state = paste(city, state, sep=", "),
    solved = ifelse(disposition == "Closed by arrest", 1, 0)) |>
  filter(
    !(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")
    ),
    victim_race %in% c("Black", "White")) |>
  mutate(
    victim_age = as.numeric(victim_age)) |>
  drop_na() |>
  mutate(
  victim_sex = fct_relevel(victim_sex, "Male"))
```
This chunk removes observations not needed for the analyses. It also mutates age as a numeric variable so the regression models can effectively use it as a continuous variable. Victim sex is releveled so that Male is the reference group. 

```{r glm for Baltimore}
#logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors
fit_logistic = 
  washington_df |>
  filter(city_state == "Baltimore, MD")|> 
  glm(solved ~ victim_sex + victim_race + victim_age, data = _, family = binomial()) 

#saving glm output and tidying it
fit_logistic |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    CI_upper = exp((estimate) + 1.96*(std.error)),
    CI_lower = exp((estimate) - 1.96*(std.error))) |>
  select(term, log_OR = estimate, OR, p.value, CI_upper, CI_lower) |> 
  knitr::kable(digits = 3)
```
In Baltimore, the odds of a homicide being solved when the victim was a male was 2.350 (95% CI: 1.793, 3.081) times the odds of a homicide being solved when the victim was a female, controlling for the victim's age and race. The beta estimate for this adjusted odds ratio is 0.854. 

```{r glm for all cities}
# glm for each of the cities in the dataset
nest_glm_city_state =
  washington_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(solved ~ victim_sex + victim_race + victim_age, data = df)),
    results = map(models, broom::tidy)) |> 
  select(-data, -models) |> 
  unnest(results)

#extracting the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims
nest_glm_city_state |> 
  select(term, estimate, city_state, std.error, p.value) |>
  mutate(
    term = fct_inorder(term),
    OR = exp(estimate),
    CI_upper = exp((estimate) + 1.96*(std.error)),
    CI_lower = exp((estimate) - 1.96*(std.error))) |> 
  select(estimate, term, city_state, OR, CI_upper, CI_lower)|>
  filter(
    term == "victim_sexFemale"
  )|>
  pivot_wider(
    names_from = term, values_from = estimate)|>
  knitr::kable(digits = 3)
```
This code chunk uses the map function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors for each of the cities. The output was saved and filtered to obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims (keeping all other variables fixed).

```{r plotting glm output for all cities}
#filtering glm output and reordering it to plot ORs for each city
nest_glm_city_state |> 
  filter(
    term == "victim_sexFemale") |>
  mutate(city_state = fct_reorder(city_state, exp(estimate))) |>
  ggplot(aes(x = city_state, y = exp(estimate))) + 
  geom_point() + 
  geom_errorbar(aes(x=city_state, 
        ymin = exp((estimate) + 1.96*(std.error)), 
        ymax = exp((estimate) - 1.96*(std.error)))) +
  theme(axis.text.x = element_text(angle = 80, hjust = 1)) +
  labs(
    title = "OR for Solving Homicides Comparing Male Victims to Female Victims by City",
    x = "City",
    y = "Estimated OR (with 95% CI)"
  )
```
The plot highlights the variety of confidence interval widths, with wider intervals occurring at the lower and higher OR values. It also makes it evident that around half of the ORs have confidence intervals that include the null value of 1, indicating that though almost all estimates are greater than 1, they are not all statistically significant. 

**Problem 3**
```{r loading dataset for problem 3}
#loading csv
raw_birthweight_df = read.csv(file = "./birthweight.csv")

#viewing variables
head(raw_birthweight_df)
```
The data set has 4,342 observations and 20 variables. 

```{r tidying data set for analysis}
#mutating mother's race, baby sex, father's race, baby length, and mother's pre-pregnancy weight for analysis
birthweight_df = raw_birthweight_df |>
  mutate(
    mrace = 
      case_match(mrace,
        1 ~ "White", 
        2 ~ "Black", 
        3 ~ "Asian",
        4 ~ "Puerto Rican",
        8 ~ "Other"),
    babysex = 
      case_match(babysex,
        1 ~ "Male",
        2 ~ "Female"),
    frace = 
      case_match(frace,
        1 ~ "White", 
        2 ~ "Black", 
        3 ~ "Asian",
        4 ~ "Puerto Rican",
        8 ~ "Other",
        9 ~ "Unknown")
  ) |>
  drop_na() |>
  mutate(
    babysex = as.factor(babysex),
    blength = as.numeric(blength),
    ppwt = as.numeric(ppwt))

str(birthweight_df)
```
This chunk tidies and manipulates the dataset so it can be analyzed in a meaningful manner. It also mutates the variables into variable types (i.e. factor and numeric variables) that allow them to be involved in the regression model. 

```{r proposing a regression model}
#proposing a model
birthweight_df = as.data.frame(birthweight_df)
birthweight_fit = 
  birthweight_df |>
  lm(bwt ~ gaweeks + ppwt + blength + blength*gaweeks, data =_)

#saving the output from the regression and putting it in a table
birthweight_fit |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    CI_upper = exp((estimate) + 1.96*(std.error)),
    CI_lower = exp((estimate) - 1.96*(std.error))) |>
  select(term, log_OR = estimate, OR, p.value, CI_upper, CI_lower) |> 
  knitr::kable(digits = 3)
```
The proposed model involves a predictor and confounder variables that were selected based on knowledge about what affects a baby's birthweight. Gestational age (in weeks) is considered the predictor variable in this model because it is amongst the most obvious of factors that contribute to the baby's weight. The mother's weight (pre-pregnancy) was included because genetics are another factor that would contribute to a baby's birthweight, and its association with both birthweight and gestational age would make it a confounder. The baby's length would also be associated with the gestational age and birthweight, making it another confounder. As there would likely be interaction between birth length and gestational age, an interaction term was included in the model. 

```{r predictions vs residuals}
#calculating residuals and predictions
birthweight_df <- birthweight_df |> 
  add_predictions(birthweight_fit) |>  
  add_residuals(birthweight_fit)|>
  mutate(type = 
           if_else(!is.na(resid), "Residual", "Prediction"))

#plotting residuals vs predictions for proposed model
birthweight_df |>
  ggplot(aes(y=resid, x=pred)) +
    geom_point() +
  labs(
    title = "Predictions vs Residuals for Proposed Model",
    x = "Predictions",
    y = "Residuals")
```
This model appears to be generally well-fitting, as there is an even spread of points across the x-axis,  but there appear to be outliers present (indicated by the points at the top left quadrant of the graph. 

```{r model using length at birth and gestational age as predictors}
#fitting model
length_age_fit = 
  birthweight_df |>
  lm(bwt ~ gaweeks + blength, data =_)

#saving output in table
length_age_fit |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    CI_upper = exp((estimate) + 1.96*(std.error)),
    CI_lower = exp((estimate) - 1.96*(std.error))) |>
  select(term, log_OR = estimate, OR, p.value, CI_upper, CI_lower) |> 
  knitr::kable(digits = 3)
```

```{r model using head circumference, length, sex, and all interactions }
#fitting model and creating interaction terms
interaction_fit = 
  birthweight_df |>
  lm(bwt ~ bhead + blength + babysex + babysex*blength + babysex*bhead + blength*bhead + blength*bhead*babysex, data =_)

#saving output in table 
interaction_fit |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    CI_upper = exp((estimate) + 1.96*(std.error)),
    CI_lower = exp((estimate) - 1.96*(std.error))) |>
  select(term, log_OR = estimate, OR, p.value, CI_upper, CI_lower) |> 
  knitr::kable(digits = 3)
```

```{r comparison in terms of the cross-validated prediction error}
cv_df =
  crossv_mc(birthweight_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df |> 
  mutate(
    birthweight_fit= map(train, \(df) lm(bwt ~ babysex + gaweeks + ppwt + blength + blength*gaweeks, data = df)),
    length_age_fit= map(train, \(df) lm(bwt ~ gaweeks + blength, data = df)),
    interaction_fit  = map(train, \(df) gam(bwt ~ bhead + blength + babysex + babysex*blength + babysex*bhead + blength*bhead + blength*bhead*babysex, data = as_tibble(df)))) |> 
  mutate(
    rmse_birthweight = map2_dbl(birthweight_fit, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_lenght_age = map2_dbl(length_age_fit, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_interaction = map2_dbl(interaction_fit, test, \(mod, df) rmse(model = mod, data = df)))
```

```{r plotting comparison}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
Model with interaction terms (including three level interaction) has more predictive accuracy. Original proposed model is a little more accurate than length + gestation  age model, but the model with three level interaction is the most accurate.
